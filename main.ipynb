{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 中文观点抽取类情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 中文观点抽取任务简介\n",
    "\n",
    "观点抽取：对于给定的文本 `d`，系统需要根据文本的内容，给出其中描述的评价对象 `a`，其中评价对象 `a` 一定在文本 `d` 中出现。数据集中每个样本是一个二元组 `<d, a>`，样例如下：\n",
    "```\n",
    "输入文本（d）：重庆老灶火锅还是很赞的，有机会可以尝试一下！\n",
    "评价对象（a）：重庆老灶火锅\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 基于抽取式MRC框架的中文观点抽取实现\n",
    "\n",
    "对于观点抽取任务，目前传统的方法使用序列标注的方法去解决。\n",
    "\n",
    "本文方法的主要思想是，将一个序列标注任务转换为一个抽取式阅读理解任务去解决。\n",
    "\n",
    "基于 `Pre-training + Fine-tuning` 模式的抽取式机器阅读理解架构如下，本文预训练模型使用的是 `ernie-gram-zh`："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/a479d066c6e340f2a0a28ab580dcc73393e072e51e1b462aae3aeb9cedb62c51)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "最关键的部分是，将观点抽取数据集转换成 `SQuAD` 兼容的格式。注意，这里的 `SQuAD` 兼容格式指的不是 `SQuAD` 原始的 `json` 文件格式，而是抽取式机器阅读理解统一的输入样本格式。\n",
    "\n",
    "以 `COTE-DP`为例，`PaddleNLP` 自带的观点抽取数据集格式如下：\n",
    "```\n",
    "{\n",
    "\t'tokens': ['重', '庆', '老', '灶', '火', '锅', '还', '是', '很', '赞', '的', '，', '有', '机', '会', '可', '以', '尝', '试', '一', '下', '！'], \n",
    "    'labels': [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], \n",
    "    'entity': '重庆老灶火锅'\n",
    "}\n",
    "```\n",
    "转换成 `SQuAD` 兼容的数据格式，即抽取式 `MRC` 模型输入样本的格式如下：\n",
    "```\n",
    "{\n",
    "\t'id': 'qid0',\n",
    "    'title': '',\n",
    "    'context': '重庆老灶火锅还是很赞的，有机会可以尝试一下！',\n",
    "    'question': '评价对象',\n",
    "    'answers': ['重庆老灶火锅'],\n",
    "    'answer_starts': [0]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将 PaddleNLP 更新到最新版本\r\n",
    "!pip install --upgrade paddlenlp -i https://mirror.baidu.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.datasets import MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(data_name='dp', split='train'):\r\n",
    "    \"\"\"根据 data_name 和 split 参数创建数据集\r\n",
    "    Args:\r\n",
    "        data_name: str, 'dp', 'bd', 'mfw'\r\n",
    "        split: str, 'train', ''test\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # 由于 COTE 数据集只提供了训练集和测试集，所以 split 参数只能是 'train' 或 'test'\r\n",
    "    assert isinstance(split, str), 'split must be str, it could be \"train\" or \"test\".'\r\n",
    "\r\n",
    "    if split == 'train':\r\n",
    "        is_test = False\r\n",
    "    elif split == 'test':\r\n",
    "        is_test = True\r\n",
    "    else:\r\n",
    "        raise ValueError('split must be \"train\" or \"test\".')\r\n",
    "\r\n",
    "    # 根据 data_name 和 split 创建数据集\r\n",
    "    dataset = load_dataset('cote', data_name, splits=[split], lazy=False)\r\n",
    "\r\n",
    "    # 下面我们将数据集转换成 SQuAD 兼容的格式\r\n",
    "    examples = []\r\n",
    "    for idx, example in enumerate(dataset):\r\n",
    "        qid = 'qid' + str(idx)\r\n",
    "        # tokens 对应 MRC 中的 context\r\n",
    "        context = ''.join(example['tokens'])\r\n",
    "        # 注意，原始的样本好多是以空格或NBSP字符开头，对于基于指针的方法这类位置敏感的方法而言\r\n",
    "        # 需要将开头的空格去掉\r\n",
    "        context = context.strip()\r\n",
    "        \r\n",
    "        # 原数据集里没有 question，需要我们自己设定一个。对于观点抽取任务的问题，\r\n",
    "        # 我们可以设为：'这句话的评价对象是什么？'\r\n",
    "        # 这里我简单的将问题设为：'评价对象'\r\n",
    "        # 问题的设定，对模型性能的影响，这里我没有做过多研究\r\n",
    "        # 感兴趣可以将 question 设定为一个不相干的问题试试看，\r\n",
    "        # 比如：'你吃过了吗？'\r\n",
    "        question = '评价对象'  \r\n",
    "        if not is_test:  # 训练集\r\n",
    "            answer = example['entity']\r\n",
    "\r\n",
    "            # 过滤掉没有答案的样本\r\n",
    "            answer_start = context.find(answer)\r\n",
    "            if answer_start < 0:\r\n",
    "                continue\r\n",
    "\r\n",
    "            new_example = {\r\n",
    "                'id': qid,\r\n",
    "                'title': '',\r\n",
    "                'context': context,\r\n",
    "                'question': question,\r\n",
    "                'answers': [answer],\r\n",
    "                'answer_starts': [answer_start]\r\n",
    "            }\r\n",
    "        else:  # 测试集   \r\n",
    "            new_example = {\r\n",
    "                'id': qid,\r\n",
    "                'title':'',\r\n",
    "                'context': context,\r\n",
    "                'question': question,\r\n",
    "                'answers': [],\r\n",
    "                'answer_starts': []\r\n",
    "            }\r\n",
    "\r\n",
    "        examples.append(new_example)\r\n",
    "    \r\n",
    "    # 根据样本列表创建一个 MapDataset 对象\r\n",
    "    dataset = MapDataset(examples)\r\n",
    "\r\n",
    "    # 返回数据集\r\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们来看一下我们创建的数据集格式是否和抽取式 `MRC` 数据集（比如，Dureader-robust）格式一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cote_dp = create_dataset('dp', split='train')\r\n",
    "train_robust = load_dataset('dureader_robust', splits='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cote_dp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_robust[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "import math\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import DataLoader\r\n",
    "from paddle.io import BatchSampler\r\n",
    "from paddle.io import DistributedBatchSampler\r\n",
    "from paddlenlp.data import Dict\r\n",
    "from paddlenlp.data import Pad\r\n",
    "from paddlenlp.data import Stack\r\n",
    "from paddlenlp.data import Tuple\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.ops.optimizer import AdamW\r\n",
    "from paddlenlp.transformers import BertForQuestionAnswering\r\n",
    "from paddlenlp.transformers import BertTokenizer\r\n",
    "from paddlenlp.transformers import ErnieForQuestionAnswering\r\n",
    "from paddlenlp.transformers import ErnieTokenizer\r\n",
    "from paddlenlp.transformers import ErnieGramForQuestionAnswering\r\n",
    "from paddlenlp.transformers import ErnieGramTokenizer\r\n",
    "from paddlenlp.transformers import RobertaForQuestionAnswering\r\n",
    "from paddlenlp.transformers import RobertaTokenizer\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from config import Config\r\n",
    "from dataset import create_dataset\r\n",
    "from utils import CrossEntropyLossForSQuAD\r\n",
    "from utils import evaluate\r\n",
    "from utils import predict\r\n",
    "from utils import prepare_train_features\r\n",
    "from utils import prepare_validation_features\r\n",
    "from utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\r\n",
    "    \"bert\": (BertForQuestionAnswering, BertTokenizer),\r\n",
    "    \"ernie\": (ErnieForQuestionAnswering, ErnieTokenizer),\r\n",
    "    \"ernie_gram\": (ErnieGramForQuestionAnswering, ErnieGramTokenizer),\r\n",
    "    \"roberta\": (RobertaForQuestionAnswering, RobertaTokenizer)\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.1 模型训与评估练代码的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train(args):\r\n",
    "    \r\n",
    "    paddle.set_device(args.device)\r\n",
    "    set_seed(args)\r\n",
    "\r\n",
    "    args.model_type = args.model_type.lower()\r\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\r\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\r\n",
    "\r\n",
    "    train_ds = create_dataset(data_name=args.data_name, split='train')\r\n",
    "    train_ds, dev_ds = train_test_split(train_ds, test_size=0.3, random_state=args.seed)\r\n",
    "    train_ds, dev_ds = MapDataset(train_ds), MapDataset(dev_ds)\r\n",
    "\r\n",
    "    train_trans_func = partial(\r\n",
    "        prepare_train_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "\r\n",
    "    train_ds.map(train_trans_func, batched=True)\r\n",
    "\r\n",
    "    dev_trans_func = partial(\r\n",
    "        prepare_validation_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "\r\n",
    "    dev_ds.map(dev_trans_func, batched=True)\r\n",
    "\r\n",
    "    # 定义BatchSampler\r\n",
    "    train_batch_sampler = DistributedBatchSampler(\r\n",
    "            dataset=train_ds, \r\n",
    "            batch_size=args.batch_size, \r\n",
    "            shuffle=True\r\n",
    "    )\r\n",
    "    dev_batch_sampler = BatchSampler(\r\n",
    "        dataset=dev_ds, \r\n",
    "        batch_size=args.batch_size, \r\n",
    "        shuffle=False\r\n",
    "    )\r\n",
    "    # 定义batchify_fn\r\n",
    "    train_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "        \"start_positions\": Stack(dtype=\"int64\"),\r\n",
    "        \"end_positions\": Stack(dtype=\"int64\")\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    dev_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    # 构造DataLoader\r\n",
    "    train_data_loader = DataLoader(\r\n",
    "        dataset=train_ds,\r\n",
    "        batch_sampler=train_batch_sampler,\r\n",
    "        collate_fn=train_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    dev_data_loader =  DataLoader(\r\n",
    "        dataset=dev_ds,\r\n",
    "        batch_sampler=dev_batch_sampler,\r\n",
    "        collate_fn=dev_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    output_dir = os.path.join(args.output_dir, 'best_model')\r\n",
    "    if not os.path.exists(output_dir):\r\n",
    "        os.makedirs(output_dir)\r\n",
    "\r\n",
    "    model = model_class.from_pretrained(args.model_name_or_path)\r\n",
    "    # model = model_class.from_pretrained(output_dir)\r\n",
    "\r\n",
    "\r\n",
    "    num_training_steps = args.max_steps if args.max_steps > 0 else len(\r\n",
    "        train_data_loader) * args.num_train_epochs\r\n",
    "    num_train_epochs = math.ceil(num_training_steps / len(train_data_loader))\r\n",
    "\r\n",
    "    num_batches = len(train_data_loader)\r\n",
    "\r\n",
    "    lr_scheduler = LinearDecayWithWarmup(\r\n",
    "        learning_rate=args.learning_rate, \r\n",
    "        total_steps=num_training_steps,\r\n",
    "        warmup=args.warmup_proportion\r\n",
    "    )\r\n",
    "\r\n",
    "    decay_params = [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ]\r\n",
    "    optimizer = paddle.optimizer.AdamW(\r\n",
    "        learning_rate=lr_scheduler,\r\n",
    "        epsilon=args.adam_epsilon,\r\n",
    "        parameters=model.parameters(),\r\n",
    "        weight_decay=args.weight_decay,\r\n",
    "        apply_decay_param_fun=lambda x: x in decay_params\r\n",
    "    )\r\n",
    "\r\n",
    "    criterion = CrossEntropyLossForSQuAD()\r\n",
    "\r\n",
    "    best_val_f1 = 0.0\r\n",
    "\r\n",
    "    global_step = 0\r\n",
    "    tic_train = time.time()\r\n",
    "    for epoch in range(1, num_train_epochs + 1):\r\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "\r\n",
    "            global_step += 1\r\n",
    "            \r\n",
    "            input_ids, segment_ids, start_positions, end_positions = batch\r\n",
    "            logits = model(input_ids=input_ids, token_type_ids=segment_ids)\r\n",
    "            loss = criterion(logits, (start_positions, end_positions))\r\n",
    "\r\n",
    "            if global_step % args.log_steps == 0 :\r\n",
    "                # print(\"global step %d, epoch: %d, batch: %d/%d, loss: %.5f,  speed: %.2f step/s\" % (\r\n",
    "                #     global_step, epoch, step, num_batches, loss, args.log_steps / (time.time() - tic_train)))\r\n",
    "                \r\n",
    "                print(\"global step %d, epoch: %d, batch: %d/%d, loss: %.5f,  speed: %.2f step/s, lr: %1.16e\" % (\r\n",
    "                    global_step, epoch, step, num_batches, loss, args.log_steps / (time.time() - tic_train), lr_scheduler.get_lr()))\r\n",
    "                \r\n",
    "                tic_train = time.time()\r\n",
    "            \r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            lr_scheduler.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "\r\n",
    "            if global_step % args.save_steps == 0 or global_step == num_training_steps:\r\n",
    "                em, f1 = evaluate(model=model, data_loader=dev_data_loader)\r\n",
    "\r\n",
    "                print(\"global step: %d, eval dev Exact Mactch: %.5f, f1_score: %.5f\" % (global_step, em, f1))\r\n",
    "\r\n",
    "                if f1 > best_val_f1:\r\n",
    "                    best_val_f1 = f1\r\n",
    "\r\n",
    "                    print(\"save model at global step: %d, best eval f1_score: %.5f\" % (global_step, best_val_f1))\r\n",
    "\r\n",
    "                    model.save_pretrained(output_dir)\r\n",
    "                    tokenizer.save_pretrained(output_dir)\r\n",
    "\r\n",
    "                if global_step == num_training_steps:\r\n",
    "                    break\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.2 模型训练参数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = Config(model_type='ernie_gram', \r\n",
    "              model_name_or_path='ernie-gram-zh', \r\n",
    "              data_name='mfw',  # dp, bd, mfw\r\n",
    "              output_dir='./outputs/cote/mfw',  # './outputs/cote/dp', './outputs/cote/bd', './outputs/cote/mfw'\r\n",
    "              \r\n",
    "              max_seq_length=128, \r\n",
    "              batch_size=32,\r\n",
    "              learning_rate=5e-5,\r\n",
    "              num_train_epochs=10,\r\n",
    "              log_steps=20,          # dp, mfw == 20,  bd == 10\r\n",
    "              save_steps=200,        # dp, mfw == 200, bd == 100\r\n",
    "              doc_stride=64,\r\n",
    "              warmup_proportion=0.1,\r\n",
    "              weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.2 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.1 模型预测代码的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_predict(args):\r\n",
    "\r\n",
    "    paddle.set_device(args.device)\r\n",
    "\r\n",
    "    output_dir = os.path.join(args.output_dir, \"best_model\")\r\n",
    "\r\n",
    "    # 1. 加载测试集\r\n",
    "    test_ds = create_dataset(data_name=args.data_name, split='test')\r\n",
    "\r\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\r\n",
    "    tokenizer = tokenizer_class.from_pretrained(output_dir)\r\n",
    "\r\n",
    "    # 2. 转化为 id\r\n",
    "    test_trans_func = partial(\r\n",
    "        prepare_validation_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "    test_ds.map(test_trans_func, batched=True)\r\n",
    "\r\n",
    "    # test BatchSampler\r\n",
    "    test_batch_sampler = BatchSampler(\r\n",
    "        dataset=test_ds, \r\n",
    "        batch_size=args.batch_size, \r\n",
    "        shuffle=False\r\n",
    "    )\r\n",
    "\r\n",
    "    # test dataset features batchify\r\n",
    "    test_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    # test DataLoader\r\n",
    "    test_data_loader =  DataLoader(\r\n",
    "        dataset=test_ds,\r\n",
    "        batch_sampler=test_batch_sampler,\r\n",
    "        collate_fn=test_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    model = model_class.from_pretrained(output_dir)\r\n",
    "    \r\n",
    "    all_predictions = predict(model, test_data_loader)\r\n",
    "\r\n",
    "    # Can also write all_nbest_json and scores_diff_json files if needed\r\n",
    "    with open('COTE_' + args.data_name.upper() + '.tsv', \"w\", encoding='utf-8') as writer:\r\n",
    "        writer.write('index\\tprediction\\n')\r\n",
    "        idx = 0\r\n",
    "        for example in test_data_loader.dataset.data:\r\n",
    "            writer.write(str(idx) + '\\t' + all_predictions[example['id']] + '\\n')\r\n",
    "            idx += 1\r\n",
    "\r\n",
    "    count = 0\r\n",
    "    for example in test_data_loader.dataset.data:\r\n",
    "        count += 1\r\n",
    "        print()\r\n",
    "        print('问题：',example['question'])\r\n",
    "        print('原文：',''.join(example['context']))\r\n",
    "        print('答案：',all_predictions[example['id']])\r\n",
    "        if count >= 10:\r\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.2 启动模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 实验结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "||DP|BD|MFW|\n",
    "|---|---|---|---|\n",
    "|Official|0.8496|0.8649|0.8732||\n",
    "|Ours|**0.913**|**0.8994**|**0.8907**|\n",
    "|diff|**0.0634**|**0.0345**|**0.0175**|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 意见反馈\n",
    "\n",
    "关于本项目有什么问题或意见可随时在NLP打卡营的 `QQ` 群里 `@我爱志方小姐`。如果您不在 `QQ` 群里里，也欢迎您在评论区留下您宝贵的建议~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
